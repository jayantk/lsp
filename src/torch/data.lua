
local path = require 'pl.path'
local C = require 'pl.comprehension' .new()
local stringx = require 'pl.stringx'
local utils = require 'pl.utils'
local re = require 're'
mk = require 'multikey'
local Set = require 'pl.Set'
local pr = require 'pl.pretty'
local torch = require 'torch'
tablex = require 'pl.tablex'

local DEFAULT_PATH="data/cobot/set/kinect/"

local DEFAULT_TRAINING_FILENAME="training.annotated.txt.merged"
local CATEGORY_FEATURE_FILE = "osm_kb.domain.entities"
local RELATION_FEATURE_FILE = "osm_kb.domain.relations"

function mysplit(inputstr, sep)
	if sep == nil then
		sep = "%s"
	end
	local t={} 
	i=1
	for str in string.gmatch(inputstr, "([^"..sep.."]+)") do
		t[i] = str
		i = i + 1
	end
	return t
end

function table_to_tensor(t, index_list)
	shape = C('tablex.size(index) for index') (index_list)
	tensor = torch.Tensor(unpack(shape))

	if #index_list == 2 then
		for _, k1, k2, v in pairs(t) do
			k = {k1, k2}
			assert(k1 ~= nil)
			assert(k2 ~= nil)
			ind = C('table(v2[k[i]] for i, v2 in ipairs(_1))')(index_list)
			tensor[ind] = v
		end
	else
		assert(#index_list == 3, #index_list)
		for _, k1, k2, k3, v in pairs(t) do
			k = {k1, k2, k3}
			ind = C('table(v2[k[i]] for i, v2 in ipairs(_1))')(index_list)
			tensor[ind] = v
		end
	end
	return tensor
end

local Domain = {}
local GroundingExample = {}

function Domain:new(
	entity_names, 
	category_feature_names, 
	relation_feature_names, 
	category_feature_matrix, 
	relation_feature_tensor, 
	training_examples)

	assert (#category_feature_names > 0)
	assert (#relation_feature_names > 0)
	t = {}
	t.entity_name_index = C('table(v, i for i, v in ipairs(_1))') (entity_names)
	t.entity_names = entity_names

	t.category_feature_names = category_feature_names 
	t.relation_feature_names = relation_feature_names 

	t.category_feature_matrix = category_feature_matrix 
	t.relation_feature_tensor = relation_feature_tensor 
	t.training_examples = training_examples 
	setmetatable(t, self)
	self.__index = self

	return t
end

function Domain:get_entity_category_feature_dict(entity_name)
	ind = self.entity_name_index[entity_name]
	vec = self.category_feature_matrix[ind]
	return C('table(self.category_feature_names[i], vec[i] for i,_ in ipairs(_1))')(self.category_feature_names)
end

function GroundingExample:new(
	tokens, 
	entity_answers, 
	answer_vector, 
	difficulty, 
	logical_form, 
	cfg_parse, 
	autogenerated_logical_form)

	t = {}
	t.tokens = tokens
	t.entity_answers = entity_answers
	t.answer_vector = answer_vector
	t.difficulty = difficulty
	t.logical_form = logical_form
	t.cfg_parse = cfg_parse
	t.autogenerated_logical_form = autogenerated_logical_form
	setmetatable(t, self)
	self.__index = self

	return t
end

function read_domains_from_directory(directory, training_filename)
	local f = io.popen("ls "..directory)
	files = mysplit(f:read("*all"), "\n")
    category_feature_list = {}
    relation_feature_list = {}
    training_data_list = {}

	for i, f in ipairs(files) do
		p = directory..f
		if path.isdir(p) then
			print(p)
            category_feature_file = p..'/'..CATEGORY_FEATURE_FILE
            relation_feature_file = p..'/'..RELATION_FEATURE_FILE
            training_data_file = p..'/'..training_filename

            category_features, relation_features, training_data_lines = read_domain(category_feature_file, relation_feature_file, training_data_file)

			table.insert(category_feature_list, category_features)
			table.insert(relation_feature_list, relation_features)
			table.insert(training_data_list, training_data_lines)
		end
	end

	category_feature_names = Set{}
    relation_feature_names = Set{}

	for i, v in pairs(category_feature_list) do
		category_feature_names = category_feature_names+Set(C('k2 for _,k1,k2,v in pairs(_1)')(category_feature_list[i]))
		relation_feature_names = relation_feature_names+Set(C('k3 for _,k1,k2,k3,v in pairs(_1)')(relation_feature_list[i]))
	end

	category_feature_name_list = Set.values(category_feature_names)
	table.sort(category_feature_name_list)
	relation_feature_name_list = Set.values(relation_feature_names)
	table.sort(relation_feature_name_list)
	category_feature_name_index = C('table(y,x for x,y in ipairs(_1))')(category_feature_name_list)
	relation_feature_name_index = C('table(y,x for x,y in ipairs(_1))')(relation_feature_name_list)

	domains = {}
	for i, category_feature in ipairs(category_feature_list) do
		entity_names = C('table(k1, true for _,k1,k2,v in pairs(_1))')(category_feature)
		entity_names = C('x for x,_ in pairs(_1)') (entity_names)
		table.sort(entity_names)
		entity_name_index = C('table(y,x for x,y in ipairs(_1))') (entity_names)
		assert(tablex.size(entity_name_index) > 0)
		assert(entity_name_index ~= nil)
		assert(category_feature_name_index ~= nil)
		assert(relation_feature_name_index ~= nil)
		category_feature_matrix = table_to_tensor(category_feature_list[i], {entity_name_index, category_feature_name_index})
		relation_feature_matrix = table_to_tensor(relation_feature_list[i], {entity_name_index, entity_name_index, relation_feature_name_index})

		assert(training_data_list[i] ~= nil)
		assert(entity_name_index ~= nil)
        training_examples = parse_training_examples(training_data_list[i], entity_name_index)
        table.insert(domains, Domain:new(
			entity_names, 
			category_feature_name_list, 
			relation_feature_name_list, 
			category_feature_matrix, 
			relation_feature_tensor, 
			training_examples))
	end
	return domains
end

function read_domain(category_feature_file, relation_feature_file, training_data_file)
    print ("   ", category_feature_file)
    print ("   ", relation_feature_file)
    print ("   ", training_data_file)

    category_features = mk()
	for line in io.lines(category_feature_file) do
		parts = utils.split(line, ',')

		entity_name = parts[1]
		feature_name = parts[3]
		value = tonumber(parts[4])

		category_features:put(entity_name, feature_name, value)
	end

    relation_features = mk()
	for line in io.lines(relation_feature_file) do
		parts = utils.split(line, ',')
		entity1_name = parts[1]
		entity2_name = parts[2]
		feature_name = parts[4]
		value = tonumber(parts[5])

		relation_features:put(entity1_name, entity2_name, feature_name, value)
	end

    training_data_lines = {}
	for line in io.lines(training_data_file) do
		table.insert(training_data_lines, line)
	end

    return category_features, relation_features, training_data_lines
end


sexp = re.compile [[
prog <- list*
list <- space {| '(' (atom/list)* space ')' |}
atom <- space { [^%s()]+ }
space <- (%s)*
]]

function parse_training_examples(training_example_lines, entity_name_index)
	examples = {}
	for _, line in ipairs(training_example_lines) do
		if stringx.startswith(line, '*') then
            print "WARNING: data contains annotated predicates"
		else
			parts = utils.split(line, ';')
			tokens = utils.split(parts[1], ' ')
			entity_answers = utils.split(parts[2], ',')
			difficulty = tonumber(parts[3])

			logical_form = nil
			cfg_parse = nil
			autogenerated_logical_form = nil

			if stringx.strip(parts[4]) ~= '' then
				logical_form = sexp:match(parts[4])
			end

			cfg_parse = sexp:match(parts[5])

			if not stringx.startswith(stringx.strip(parts[6]), 'NO') then
				autogenerated_logical_form = sexp:match(parts[6])
			end

			answer_vector = torch.Tensor(tablex.size(entity_name_index))

			for i, entity in pairs(entity_answers) do
				answer_vector[entity_name_index[entity]]  = 1.0
			end

			table.insert(examples, GroundingExample:new(tokens, entity_answers, answer_vector, difficulty, logical_form, cfg_parse, autogenerated_logical_form))
		end
	end

	return examples
end

-- domains = read_domains_from_directory(DEFAULT_PATH, DEFAULT_TRAINING_FILENAME)
-- pr.dump(domains)
